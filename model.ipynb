{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f38aa9f8-b69c-4c46-b247-0e8831b1a8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\Lenovo\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import RFE\n",
    "from lightgbm import LGBMClassifier\n",
    "import itertools\n",
    "import joblib\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.sklearn import SKLearn\n",
    "from sagemaker import Session\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import yaml\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from pathlib import Path\n",
    "from timeutils import Stopwatch\n",
    "import logging\n",
    "import argparse\n",
    "import tarfile\n",
    "import warnings\n",
    "import optuna\n",
    "# Initialize logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5efe49c9-f203-48c0-ace3-6f3a2c7d2a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training job...\n",
      "Loading data...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 167\u001b[0m\n\u001b[0;32m    164\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Load the training data (including target variable)\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m train_file \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrain_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m x_train, y_train \u001b[38;5;241m=\u001b[39m get_data(train_file, target_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Load the test data (without target variable)\u001b[39;00m\n",
      "File \u001b[1;32m<frozen ntpath>:108\u001b[0m, in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data by handling missing values, label encoding, and dropping unnecessary columns.\n",
    "    \"\"\"\n",
    "    # Handling missing values and removing duplicate rows\n",
    "    total = df.shape[0]\n",
    "    missing_columns = [col for col in df.columns if df[col].isnull().sum() > 0]\n",
    "    for col in missing_columns:\n",
    "        null_count = df[col].isnull().sum()\n",
    "        per = (null_count / total) * 100\n",
    "        print(f\"{col}: {null_count} ({round(per, 3)}%)\")\n",
    "\n",
    "    print(f\"Number of duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "    # Drop duplicate rows\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Initialize label encoders for categorical features\n",
    "    encoding_dict = {\"protocol_type\": LabelEncoder(), \"service\": LabelEncoder(), \"flag\": LabelEncoder()}\n",
    "\n",
    "    # Convert categorical columns to numeric using label encoding\n",
    "    categorical_columns = ['protocol_type', 'service', 'flag']\n",
    "    for column in categorical_columns:\n",
    "        df[column] = df[column].astype('category').cat.codes  # Convert to category codes\n",
    "\n",
    "    # Perform label encoding on any remaining object columns\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            label_encoder = LabelEncoder()\n",
    "            df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    if 'num_outbound_cmds' in df.columns:\n",
    "        df.drop(['num_outbound_cmds'], axis=1, inplace=True)\n",
    "\n",
    "    # Save the encoders to a file\n",
    "    joblib.dump(encoding_dict, 'encoders.joblib')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_selection(df, target_column='class'):\n",
    "    \"\"\"\n",
    "    Perform feature selection to separate features and target variable.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): The DataFrame to process.\n",
    "    target_column (str): The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the features (X) and labels (y) if target_column exists.\n",
    "    \"\"\"\n",
    "    if target_column in df.columns:\n",
    "        X = df.drop([target_column], axis=1)  # Features\n",
    "        y = df[target_column]  # Target variable\n",
    "        return X, y\n",
    "    else:\n",
    "        X = df  # Features\n",
    "        return X, None  # No target variable\n",
    "\n",
    "\n",
    "def scale_and_split_data(X, y):\n",
    "    \"\"\"\n",
    "    Scale the data and split it into training and testing sets.\n",
    "    \"\"\"\n",
    "    # Initialize the scaler\n",
    "    scale = StandardScaler()\n",
    "    \n",
    "    # Scale the features\n",
    "    X_scaled = scale.fit_transform(X)\n",
    "    \n",
    "    # Split the data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_scaled, y, train_size=0.70, random_state=2)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def get_data(file_path, target_column=None):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file and split into features (X) and labels (y).\n",
    "\n",
    "    Args:\n",
    "    file_path (str): The full path to the CSV file, can be a local path or S3 URI.\n",
    "    target_column (str): The name of the target column (for training data).\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the features (X) and labels (y) if target_column exists.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to read file: {file_path}\")\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    data = preprocess_data(data)\n",
    "    \n",
    "    if target_column and target_column in data.columns:\n",
    "        X, y = feature_selection(data, target_column)\n",
    "    else:\n",
    "        X, y = feature_selection(data)  # No target column in test data\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_lgbm(x_train, y_train, x_test, y_test, random_state=42):\n",
    "    \"\"\"\n",
    "    Trains a LightGBM model and evaluates it on training and test data.\n",
    "\n",
    "    Args:\n",
    "    x_train (pd.DataFrame or np.array): Features for training.\n",
    "    y_train (pd.Series or np.array): Labels for training.\n",
    "    x_test (pd.DataFrame or np.array): Features for testing.\n",
    "    y_test (pd.Series or np.array): Labels for testing.\n",
    "    random_state (int): Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the trained model and a dictionary with the training and test scores.\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    lgb_model = LGBMClassifier(random_state=random_state)\n",
    "    \n",
    "    # Train the model\n",
    "    lgb_model.fit(x_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    lgb_train_score = lgb_model.score(x_train, y_train)\n",
    "    \n",
    "    scores = {\"Training Score\": lgb_train_score}\n",
    "    \n",
    "    if y_test is not None:\n",
    "        lgb_test_score = lgb_model.score(x_test, y_test)\n",
    "        scores[\"Test Score\"] = lgb_test_score\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Training Score: {lgb_train_score}\")\n",
    "    if y_test is not None:\n",
    "        print(f\"Test Score: {lgb_test_score}\")\n",
    "    \n",
    "    # Return the model and scores in a tuple\n",
    "    return lgb_model, scores\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Starting training job...\")\n",
    "    logger.debug(\"Starting training job...\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters sent by the client are passed as command-line arguments to the script\n",
    "    parser.add_argument('--random_state', type=int, default=42)\n",
    "    \n",
    "    # Data directories (can be local or S3 paths)\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    \n",
    "    # Model directory: SM_MODEL_DIR is always set to /opt/ml/model\n",
    "    parser.add_argument('--sm_model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    \n",
    "    parser.add_argument('--gpu-count', type=int, default=os.environ.get('SM_NUM_GPUS'))\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    print(\"Loading data...\")\n",
    "    logger.debug(\"Loading data...\")\n",
    "    \n",
    "    # Load the training data (including target variable)\n",
    "    train_file = os.path.join(args.train, 'Train_data.csv')\n",
    "    x_train, y_train = get_data(train_file, target_column='class')\n",
    "    \n",
    "    # Load the test data (without target variable)\n",
    "    test_file = os.path.join(args.test, 'Test_data.csv')\n",
    "    x_test, _ = get_data(test_file)  # Test data won't have target variable\n",
    "    \n",
    "    if y_train is None:\n",
    "        raise ValueError(\"Training data must contain the target column.\")\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    print(\"Training model...\")\n",
    "    logger.debug(\"Training model...\")\n",
    "    model, scores = train_and_evaluate_lgbm(x_train, y_train, x_test, None, random_state=args.random_state)\n",
    "    \n",
    "    print(f\"Training Score: {scores['Training Score']}\", end='')\n",
    "    if \"Test Score\" in scores:\n",
    "        print(f\", Test Score: {scores['Test Score']}\")\n",
    "    else:\n",
    "        print()  # If no test score available, print a new line\n",
    "    \n",
    "    logger.debug(f\"Training Score: {scores['Training Score']}\", end='')\n",
    "    if \"Test Score\" in scores:\n",
    "        logger.debug(f\", Test Score: {scores['Test Score']}\")\n",
    "    else:\n",
    "        logger.debug(\"\")\n",
    "    \n",
    "    # Save the model\n",
    "    print(\"Saving model...\")\n",
    "    logger.debug(\"Saving model...\")\n",
    "\n",
    "    # # 2 Save the model locally\n",
    "    # local_model_path = os.path.join('/opt/ml/model', 'model.joblib')\n",
    "    # joblib.dump(model, local_model_path)\n",
    "    \n",
    "     joblib.dump(model, os.path.join(args.sm_model_dir, 'model.joblib'))\n",
    "    # Save the model to the SageMaker model directory or a local path\n",
    "    \n",
    "    # #3 Check if running in a SageMaker environment\n",
    "    # if args.sm_model_dir.startswith(\"/opt/ml/\"):\n",
    "    #     # Use SageMaker-provided model directory\n",
    "    #     model_path = os.path.join(args.sm_model_dir, 'model.joblib')\n",
    "    # else:\n",
    "    #     # Running locally, use a local path\n",
    "    #     if not os.path.exists(args.sm_model_dir):\n",
    "    #         os.makedirs(args.sm_model_dir)\n",
    "    #     model_path = os.path.join(args.sm_model_dir, 'model.joblib')\n",
    "    \n",
    "    # # Save the model\n",
    "    # joblib.dump(model, model_path)\n",
    "    \n",
    "    # print(f\"Model saved at {model_path}.\")\n",
    "    # logger.debug(f\"Model saved at {model_path}.\")\n",
    "    \n",
    "    # #4 Local model directory\n",
    "    # local_model_dir = '/opt/ml/model'\n",
    "    # local_model_path = os.path.join(local_model_dir, 'model.joblib')\n",
    "    \n",
    "    # os.makedirs(local_model_dir, exist_ok=True)\n",
    "    # joblib.dump(model, local_model_path)\n",
    "    \n",
    "    # # Upload to S3\n",
    "    # s3_client = boto3.client('s3')\n",
    "    # s3_bucket = args.sm_model_dir.split('/')[2]\n",
    "    # s3_prefix = '/'.join(args.sm_model_dir.split('/')[3:])\n",
    "    # s3_model_path = os.path.join(s3_prefix, 'model.joblib')\n",
    "    \n",
    "    # print(\"Uploading model to S3...\")\n",
    "    # s3_client.upload_file(local_model_path, s3_bucket, s3_model_path)\n",
    "    \n",
    "    # print(f\"Model successfully uploaded to: s3://{s3_bucket}/{s3_model_path}\")\n",
    "    # logger.debug(f\"Model successfully uploaded to: s3://{s3_bucket}/{s3_model_path}\")    \n",
    "\n",
    "    # #5 Define local model directory\n",
    "    # local_model_dir = '/opt/ml/model'  # SageMaker's default model directory\n",
    "    # local_model_path = os.path.join(local_model_dir, 'model.joblib')\n",
    "    \n",
    "    # # Save the model locally\n",
    "    # os.makedirs(local_model_dir, exist_ok=True)  # Ensure the local directory exists\n",
    "    # joblib.dump(model, local_model_path)\n",
    "    \n",
    "    # # Compress the model into a tar file\n",
    "    # tar_path = os.path.join(local_model_dir, 'model.tar.gz')\n",
    "    \n",
    "    # with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "    #     tar.add(local_model_path, arcname='model.joblib')\n",
    "    \n",
    "    # # Upload the tar file to S3\n",
    "    # s3_client = boto3.client('s3')\n",
    "    \n",
    "    # # Extract the bucket and prefix from the S3 URI\n",
    "    # s3_bucket = args.sm_model_dir.split('/')[2]\n",
    "    # s3_prefix = '/'.join(args.sm_model_dir.split('/')[3:])\n",
    "    \n",
    "    # # Define the final S3 path for the model tar file\n",
    "    # s3_model_tar_path = os.path.join(s3_prefix, 'model.tar.gz')\n",
    "    \n",
    "    # # Upload the tar file to S3\n",
    "    # print(\"Uploading model tar file to S3...\")\n",
    "    # s3_client.upload_file(tar_path, s3_bucket, s3_model_tar_path)\n",
    "    \n",
    "    # print(f\"Model tar file successfully uploaded to: s3://{s3_bucket}/{s3_model_tar_path}\")\n",
    "    \n",
    "    # print(\"Model saved.\")\n",
    "    # logger.debug(\"Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baabb3e0-98f3-43bf-91e5-173142978283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
